import Layout from '~/layouts/DefaultGuideLayout'

export const meta = {
  id: 'storage-schema-optimisations',
  title: 'Storage Optimisations',
  description: 'Learn how to scale Storage',
  subtitle: 'Learn how to scale Storage',
  sidebar_label: 'Schema',
}

Here are some optimisations that you can consider to improve performance and reduce costs as you start scaling Storage.

## Egress

If your project has high egress, these are some optimisations that will help you reduce it.

#### Resize Images

Images usually contribute to the majority of your egress. By keeping their size as small as possible, you can reduce your egress and increase the performance of your application at the same time.
You can use our [Image Transformation](/docs/guides/storage/serving/image-transformations) to optimise any image on the fly.

#### Set a high cache-control value

Utilising the browser cache will help reduce your egress as the asset will be stored on the user's browser after it is first downloaded. Setting a high `cache-control` value will ensure that the asset is stored on the user's browser for a long time. This will reduce the number of times the asset is downloaded from the server.

#### Limit the upload size

You can set the maximum upload size for your bucket. This will prevent users from uploading and subsequently downloading large files. You can limit the max file size by setting this option at the [bucket level](/docs/guides/storage/buckets/creating-buckets)

## Optimize Listing Objects

When you reach a high number of objects you might be notice that `supabase.storage.list()` method starts getting slow.

This is because the endpoint is very generic and tries to compute folders and objects in a single query.
It is very useful when building something like the Storage viewer on the Supabase dashboard.

However, if your application don't need the entire hierarchy computed you can speed up drastically the query execution for listing your objects by creating a Postgres function as following:

```sql
create or replace function list_objects(
    bucketid text,
    prefix text,
    limits int default 100,
    offsets int default 0
) returns table (
    name text,
    id uuid,
    updated_at timestamptz,
    created_at timestamptz,
    last_accessed_at timestamptz,
    metadata jsonb
) as $$
begin
    return query SELECT
        objects.name,
        objects.id,
        objects.updated_at,
        objects.created_at,
        objects.last_accessed_at,
        objects.metadata
    FROM storage.objects
    WHERE objects.name like prefix || '%'
    AND bucket_id = bucketid
    ORDER BY name ASC
    LIMIT limits
    OFFSET offsets;
end;
$$ language plpgsql stable;
```

You can then use the your Postgres function as following:

Using SQL:

```sql
select * from list_objects('bucket_id', '', 100, 0);
```

Using the SDK:

```js
const { data, error } = await supabase.rpc('list_objects', {
  bucketid: 'yourbucket',
  prefix: '',
  limit: 100,
  offset: 0,
})
```

## Optimising RLS

When creating RLS policies against the storage tables you can add indexes to the interested columns to speed up the lookup

export const Page = ({ children }) => <Layout meta={meta} children={children} />

export default Page
